{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wFlSHS62ooNW"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "# YOLOv8 simple object detector using a class-based approach\n",
        "\n",
        "</div>\n",
        "\n",
        "The YOLOv8 is a powerful object detection algorithm that combines high accuracy and real-time detection speeds. This Colab notebook demonstrates how to implement a simple object detector using a class-based approach, allowing us to detect objects in both static images and videos.\n",
        "\n",
        "By using a class-based approach, the notebook will offer several advantages over other approaches, such as easier maintenance and extensibility of the code. The code will also be more modular and easier to understand, making it accessible to a wider audience of users."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoZrnFsqLDR6"
      },
      "source": [
        "## Importing libraries, modules and files"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ymb1wGDIo40f"
      },
      "source": [
        "### Importing modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FDIvbG71Jkhn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "import time\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLCrvX5uxtAt",
        "outputId": "7b85e1d8-c00e-4e5f-b83e-edd18af0bc30"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.0.59 ðŸš€ Python-3.9.16 torch-1.13.1+cu116 CPU\n",
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 26.4/107.7 GB disk)\n"
          ]
        }
      ],
      "source": [
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()\n",
        "from ultralytics import YOLO\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aUpE3ZjhpRsA"
      },
      "source": [
        "## Object detection implementation:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "v-u9ZFK8pCzq"
      },
      "source": [
        "### Defining the YOLOv8_ObjectDetector Class for object detection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qI-I-BBS81Fa"
      },
      "outputs": [],
      "source": [
        "class YOLOv8_ObjectDetector:\n",
        "    \"\"\"\n",
        "    A class for performing object detection on images and videos using YOLOv8.\n",
        "\n",
        "    Args:\n",
        "    ------------\n",
        "        model_file (str): Path to the YOLOv8 model file or yolo model variant name in ths format: [variant].pt\n",
        "        labels (list[str], optional): A list of class labels for the model. If None, uses the default labels from the model file.\n",
        "        classes (list[str], optional): Alias for labels. Deprecated.\n",
        "        conf (float, optional): Minimum confidence threshold for object detection.\n",
        "        iou (float, optional): Minimum IOU threshold for non-max suppression.\n",
        "\n",
        "    Attributes:\n",
        "    --------------\n",
        "        classes (list[str]): A list of class labels for the model ( a Dict is also acceptable).\n",
        "        conf (float): Minimum confidence threshold for object detection.\n",
        "        iou (float): Minimum IOU threshold for non-max suppression.\n",
        "        model (YOLO): The YOLOv8 model used for object detection.\n",
        "        model_name (str): The name of the YOLOv8 model file (without the .pt extension).\n",
        "\n",
        "    Methods :\n",
        "    -------------\n",
        "        default_display: Returns a default display (ultralytics plot implementation) of the object detection results.\n",
        "        custom_display: Returns a custom display of the object detection results.\n",
        "        predict_video: Predicts objects in a video and saves the results to a file.\n",
        "        predict_img: Predicts objects in an image and returns the detection results.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_file = 'yolov8n.pt', labels= None, classes = None, conf = 0.25, iou = 0.45 ):\n",
        "\n",
        "        self.classes = classes\n",
        "        self.conf = conf\n",
        "        self.iou = iou\n",
        "\n",
        "        self.model = YOLO(model_file)\n",
        "        self.model_name = model_file.split('.')[0]\n",
        "        self.results = None\n",
        "\n",
        "        if labels == None:\n",
        "            self.labels = self.model.names\n",
        "\n",
        "    def predict_img(self, img, verbose=True):\n",
        "        \"\"\"\n",
        "        Runs object detection on a single image.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "            img (numpy.ndarray): Input image to perform object detection on.\n",
        "            verbose (bool): Whether to print detection details.\n",
        "\n",
        "        Returns:\n",
        "        -----------\n",
        "            'ultralytics.yolo.engine.results.Results': A YOLO results object that contains \n",
        "             details about detection results :\n",
        "                    - Class IDs\n",
        "                    - Bounding Boxes\n",
        "                    - Confidence score\n",
        "                    ...\n",
        "        (pls refer to https://docs.ultralytics.com/reference/results/#results-api-reference for results API reference)\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Run the model on the input image with the given parameters\n",
        "        results = self.model(img, classes=self.classes, conf=self.conf, iou=self.iou, verbose=verbose)\n",
        "\n",
        "        # Save the original image and the results for further analysis if needed\n",
        "        self.orig_img = img\n",
        "        self.results = results[0]\n",
        "\n",
        "        # Return the detection results\n",
        "        return results[0]\n",
        "\n",
        "\n",
        "\n",
        "    def default_display(self, show_conf=True, line_width=None, font_size=None, \n",
        "                        font='Arial.ttf', pil=False, example='abc'):\n",
        "        \"\"\"\n",
        "        Displays the detected objects on the original input image.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        show_conf : bool, optional\n",
        "            Whether to show the confidence score of each detected object, by default True.\n",
        "        line_width : int, optional\n",
        "            The thickness of the bounding box line in pixels, by default None.\n",
        "        font_size : int, optional\n",
        "            The font size of the text label for each detected object, by default None.\n",
        "        font : str, optional\n",
        "            The font type of the text label for each detected object, by default 'Arial.ttf'.\n",
        "        pil : bool, optional\n",
        "            Whether to return a PIL Image object, by default False.\n",
        "        example : str, optional\n",
        "            A string to display on the example bounding box, by default 'abc'.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        numpy.ndarray or PIL Image\n",
        "            The original input image with the detected objects displayed as bounding boxes.\n",
        "            If `pil=True`, a PIL Image object is returned instead.\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        ValueError\n",
        "            If the input image has not been detected by calling the `predict_img()` method first.\n",
        "        \"\"\"\n",
        "        # Check if the `predict_img()` method has been called before displaying the detected objects\n",
        "        if self.results is None:\n",
        "            raise ValueError('No detected objects to display. Call predict_img() method first.')\n",
        "        \n",
        "        # Call the plot() method of the `self.results` object to display the detected objects on the original image\n",
        "        display_img = self.results.plot(show_conf, line_width, font_size, font, pil, example)\n",
        "        \n",
        "        # Return the displayed image\n",
        "        return display_img\n",
        "\n",
        "        \n",
        "\n",
        "    def custom_display(self, colors, show_cls = True, show_conf = True):\n",
        "        \"\"\"\n",
        "        Custom display method that draws bounding boxes and labels on the original image, \n",
        "        with additional options for showing class and confidence information.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        colors : list\n",
        "            A list of tuples specifying the color of each class.\n",
        "        show_cls : bool, optional\n",
        "            Whether to show class information in the label text. Default is True.\n",
        "        show_conf : bool, optional\n",
        "            Whether to show confidence information in the label text. Default is True.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        numpy.ndarray\n",
        "            The image with bounding boxes and labels drawn on it.\n",
        "        \"\"\"\n",
        "\n",
        "        img = self.orig_img\n",
        "        # calculate the bounding box thickness based on the image width and height\n",
        "        bbx_thickness = (img.shape[0] + img.shape[1]) // 450\n",
        "\n",
        "        for box in self.results.boxes:\n",
        "            textString = \"\"\n",
        "\n",
        "            # Extract object class and confidence score\n",
        "            score = box.conf.item() * 100\n",
        "            class_id = int(box.cls.item())\n",
        "\n",
        "            x1 , y1 , x2, y2 = np.squeeze(box.xyxy.numpy()).astype(int)\n",
        "\n",
        "            # Print detection info\n",
        "            if show_cls:\n",
        "                textString += f\"{self.labels[class_id]}\"\n",
        "\n",
        "            if show_conf:\n",
        "                textString += f\" {score:,.2f}%\"\n",
        "\n",
        "            # Calculate font scale based on object size\n",
        "            font = cv2.FONT_HERSHEY_COMPLEX\n",
        "            fontScale = (((x2 - x1) / img.shape[0]) + ((y2 - y1) / img.shape[1])) / 2 * 2.5\n",
        "            fontThickness = 1\n",
        "            textSize, baseline = cv2.getTextSize(textString, font, fontScale, fontThickness)\n",
        "\n",
        "            # Draw bounding box, a centroid and label on the image\n",
        "            img = cv2.rectangle(img, (x1,y1), (x2,y2), colors[class_id], bbx_thickness)\n",
        "            center_coordinates = ((x1 + x2)//2, (y1 + y2) // 2)\n",
        "\n",
        "            img =  cv2.circle(img, center_coordinates, 5 , (0,0,255), -1)\n",
        "            \n",
        "             # If there are no details to show on the image\n",
        "            if textString != \"\":\n",
        "                if (y1 < textSize[1]):\n",
        "                    y1 = y1 + textSize[1]\n",
        "                else:\n",
        "                    y1 -= 2\n",
        "                # show the details text in a filled rectangle\n",
        "                img = cv2.rectangle(img, (x1, y1), (x1 + textSize[0] , y1 -  textSize[1]), colors[class_id], cv2.FILLED)\n",
        "                img = cv2.putText(img, textString , \n",
        "                    (x1, y1), font, \n",
        "                    fontScale,  (0, 0, 0), fontThickness, cv2.LINE_AA)\n",
        "\n",
        "        return img\n",
        "\n",
        "\n",
        "    def predict_video(self, video_path, save_dir, save_format=\"avi\", display='custom', verbose=True, **display_args):\n",
        "        \"\"\"Runs object detection on each frame of a video and saves the output to a new video file.\n",
        "\n",
        "        Args:\n",
        "        ----------\n",
        "            video_path (str): The path to the input video file.\n",
        "            save_dir (str): The path to the directory where the output video file will be saved.\n",
        "            save_format (str, optional): The format for the output video file. Defaults to \"avi\".\n",
        "            display (str, optional): The type of display for the detection results. Defaults to 'custom'.\n",
        "            verbose (bool, optional): Whether to print information about the video file and output file. Defaults to True.\n",
        "            **display_args: Additional arguments to be passed to the display function.\n",
        "\n",
        "        Returns:\n",
        "        ------------\n",
        "            None\n",
        "        \"\"\"\n",
        "        # Open the input video file\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "        # Get the name of the input video file\n",
        "        vid_name = os.path.basename(video_path)\n",
        "\n",
        "        # Get the dimensions of each frame in the input video file\n",
        "        width = int(cap.get(3))  # get `width`\n",
        "        height = int(cap.get(4))  # get `height`\n",
        "\n",
        "        # Create the directory for the output video file if it does not already exist\n",
        "        if not os.path.isdir(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "\n",
        "        # Set the name and path for the output video file\n",
        "        save_name = self.model_name + ' -- ' + vid_name.split('.')[0] + '.' + save_format\n",
        "        save_file = os.path.join(save_dir, save_name)\n",
        "\n",
        "        # Print information about the input and output video files if verbose is True\n",
        "        if verbose:\n",
        "            print(\"----------------------------\")\n",
        "            print(f\"DETECTING OBJECTS IN : {vid_name} : \")\n",
        "            print(f\"RESOLUTION : {width}x{height}\")\n",
        "            print('SAVING TO :' + save_file)\n",
        "\n",
        "        # Define an output VideoWriter object\n",
        "        out = cv2.VideoWriter(save_file,\n",
        "                              cv2.VideoWriter_fourcc(*\"MJPG\"),\n",
        "                              30, (width, height))\n",
        "\n",
        "        # Check if the input video file was opened correctly\n",
        "        if not cap.isOpened():\n",
        "            print(\"Error opening video stream or file\")\n",
        "\n",
        "        # Read each frame of the input video file\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "\n",
        "            # If the frame was not read successfully, break the loop\n",
        "            if not ret:\n",
        "                print(\"Error reading frame\")\n",
        "                break\n",
        "\n",
        "            # Run object detection on the frame and calculate FPS\n",
        "            beg = time.time()\n",
        "            results = self.predict_img(frame, verbose=False)\n",
        "            if results is None:\n",
        "                print('***********************************************')\n",
        "            fps = 1 / (time.time() - beg)\n",
        "\n",
        "            # Display the detection results\n",
        "            if display == 'default':\n",
        "                frame = self.default_display(**display_args)\n",
        "            elif display == 'custom':\n",
        "                frame == self.custom_display(**display_args)\n",
        "\n",
        "            # Display the FPS on the frame\n",
        "            frame = cv2.putText(frame, f\"FPS : {fps:,.2f}\",\n",
        "                                (5, 15), cv2.FONT_HERSHEY_COMPLEX,\n",
        "                                0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
        "\n",
        "            # Write the frame to the output video file\n",
        "            out.write(frame)\n",
        "\n",
        "            # Exit the loop if the 'q' button is pressed\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break\n",
        "\n",
        "        # After the loop release the cap and video writer\n",
        "        cap.release()\n",
        "        out.release()\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AOY6Es0ulzQ",
        "outputId": "c456bf3e-cbe6-49af-aeb8-2afb95a9824d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n"
          ]
        }
      ],
      "source": [
        "d = YOLOv8_ObjectDetector()\n",
        "print(d.labels) "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FCzFGoHzpb49"
      },
      "source": [
        "### Preparing directories and file-paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Y-xXMgUjiX_0"
      },
      "outputs": [],
      "source": [
        "img_results_path = '/content/image_object_detection_results'\n",
        "vid_results_path = '/content/video_object_detection_results'\n",
        "test_imgs_path = '/content/Object-tracking-and-counting-using-YOLOV8/test imgs'\n",
        "test_vids_path = '/content/Object-tracking-and-counting-using-YOLOV8/test vids'\n",
        "\n",
        "if not os.path.isdir(img_results_path):\n",
        "    os.makedirs(img_results_path)\n",
        "\n",
        "if not os.path.isdir(vid_results_path):\n",
        "    os.makedirs(vid_results_path)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1qaxMVZpplM8"
      },
      "source": [
        "### Instantiating object detectors with different YOLOv8 model variants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YhaMvVPHAy1r"
      },
      "outputs": [],
      "source": [
        "yolo_names = ['yolov8n.pt', 'yolov8m.pt']#, 'yolov8l.pt', 'yolov8x.pt']\n",
        "colors = []\n",
        "for _ in range(80):\n",
        "    rand_tuple = (random.randint(50, 255), random.randint(50, 255), random.randint(50, 255))\n",
        "    colors.append(rand_tuple)\n",
        "\n",
        "detectors = []\n",
        "for yolo_name in yolo_names:\n",
        "    detector = YOLOv8_ObjectDetector(yolo_name, conf = 0.55 )\n",
        "    detectors.append(detector)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8CvqND8XpyoH"
      },
      "source": [
        "### Performing object detection on static images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hn6Zjv37oVnP"
      },
      "outputs": [],
      "source": [
        "for detector in detectors :\n",
        "    print(f\"Model : {detector.model_name}\")\n",
        "    for img_name in os.listdir(test_imgs_path):\n",
        "        print(f\"Target image : {img_name}\")\n",
        "        print(img_name.split('.')[0])\n",
        "        print(\"-------------------------\")\n",
        "        img = cv2.imread(os.path.join(test_imgs_path, img_name))\n",
        "        results = detector.predict_img(img, verbose = False)\n",
        "        result_img = detector.custom_display(colors = colors)\n",
        "\n",
        "        save_dir = os.path.join(img_results_path, detector.model_name)\n",
        "        if not os.path.isdir(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "\n",
        "        save_name = detector.model_name + ' -- ' + img_name.split('.')[0] + '.jpg'\n",
        "        save_file = os.path.join(img_results_path, save_dir, save_name) \n",
        "        print(f\"Saving results to : {save_file}\")\n",
        "        cv2.imwrite(save_file, result_img)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iyZpNRyhp5tW"
      },
      "source": [
        "### Performing object detection on a video file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2pBdBNpBPWm",
        "outputId": "a21547fc-d7f5-4c42-a5ad-c2784f54e672"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------\n",
            "DETECTING OBJECTS IN : traffic 2.mp4 : \n",
            "RESOLUTION : 1280x720\n",
            "SAVING TO :/content/video_object_detection_results/yolov8n -- traffic 2.avi\n",
            "Error reading frame\n",
            "----------------------------\n",
            "DETECTING OBJECTS IN : traffic 2.mp4 : \n",
            "RESOLUTION : 1280x720\n",
            "SAVING TO :/content/video_object_detection_results/yolov8m -- traffic 2.avi\n",
            "Error reading frame\n"
          ]
        }
      ],
      "source": [
        "for detector in detectors:\n",
        "    detector.predict_video(video_path= '/content/Object-tracking-and-counting-using-YOLOV8/test vids/traffic 2.mp4'\n",
        ", save_dir = vid_results_path, save_format = \"avi\", display = 'custom', colors = colors)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iBtUSozBqBoM"
      },
      "source": [
        "## Preparing results for download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NssC0XolnGk_"
      },
      "outputs": [],
      "source": [
        "!zip -r img_results.zip /content/image_object_detection_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3GYF0xsxd4h",
        "outputId": "80f59d91-def0-4da3-8406-0a55351cf283"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: content/video_object_detection_results/ (stored 0%)\n",
            "  adding: content/video_object_detection_results/yolov8n -- traffic 2.avi (deflated 0%)\n",
            "  adding: content/video_object_detection_results/yolov8m -- traffic 2.avi (deflated 0%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r vid_results.zip /content/video_object_detection_results"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
